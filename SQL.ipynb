{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb7f722",
   "metadata": {},
   "source": [
    "# Apache Beam - SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a283caf",
   "metadata": {},
   "source": [
    "We can code SQL in Beam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a7cef",
   "metadata": {},
   "source": [
    "Since our notebook is going to use Google Cloud SDK JARS we must include these in our dependencies.  Specifically, we need to include:\n",
    "\n",
    "```\n",
    "<dependency>\n",
    "  <groupId>org.apache.beam</groupId>\n",
    "  <artifactId>beam-sdks-java-io-google-cloud-platform</artifactId>\n",
    "  <version>2.43.0</version>\n",
    "</dependency>\n",
    "```\n",
    "\n",
    "Normally we would load our dependencies using the IJava Jupyter cell magic called `%%loadFromPom`.  Unfortunately, this doesn't work ([issue](https://github.com/SpencerPark/IJava/issues/139)).  A workaround is to download the dependencies outside of Jupyter and then launch Jupyter with the downloaded dependencies in the classpath.\n",
    "\n",
    "```\n",
    "mvn dependency:copy-dependencies\n",
    "export IJAVA_CLASSPATH=\"./target/dependency/*\"\n",
    "jupyter notebook\n",
    "\n",
    "```\n",
    "\n",
    "When we think about coding SQL in Beam, a question arises as to *which* SQL?  What is the syntax/structure of the dialect of SQL we should use?  Beam provides two flavors of SQL ... Calcite and Zeta.  The default is Calcite.  We can change which flavor is used through the PipelineOptions ([BeamSqlPipelineOptions](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/extensions/sql/impl/BeamSqlPipelineOptions.html#setPlannerName-java.lang.String-)) and the `setPlannerName()` method.  If we pass in `org.apache.beam.sdk.extensions.sql.zetasql.ZetaSQLQueryPlanner` we will switch to Zeta.\n",
    "\n",
    "See also:\n",
    "* [Beam SQL Overview](https://beam.apache.org/documentation/dsls/sql/overview/)\n",
    "* [Exploring Beam SQL on Google Cloud Platform](https://servian.dev/exploring-beam-sql-on-google-cloud-platform-b6c77f9b4af4)\n",
    "* [Data processing with Dataflow SQL (part 1/2)](https://medium.com/syntio/data-processing-with-dataflow-sql-part-1-2-fe57e47f4bb0)\n",
    "* [Data processing with Dataflow SQL (part 2/2)](https://medium.com/syntio/data-processing-with-dataflow-sql-part-2-2-3f1d507b6297)\n",
    "* [JavaDoc: Row](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/values/Row.html)\n",
    "* [JavaDoc: Schema](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/schemas/Schema.html)\n",
    "* [JavaDoc: SqlTransform](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/extensions/sql/SqlTransform.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211db4d",
   "metadata": {},
   "source": [
    "Next we define our imports required for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a00821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.PipelineResult;\n",
    "import org.apache.beam.sdk.options.Default;\n",
    "import org.apache.beam.sdk.options.Description;\n",
    "import org.apache.beam.sdk.options.PipelineOptionsFactory;\n",
    "import org.apache.beam.sdk.options.PipelineOptions;\n",
    "import org.apache.beam.sdk.options.StreamingOptions;\n",
    "import org.apache.beam.runners.direct.DirectOptions;\n",
    "import org.apache.beam.sdk.transforms.Create;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.Sample;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.coders.KvCoder;\n",
    "import org.apache.beam.sdk.coders.StringUtf8Coder;\n",
    "import org.apache.beam.sdk.schemas.Schema;\n",
    "import org.apache.beam.sdk.values.Row;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.KV;\n",
    "import org.apache.beam.sdk.values.TypeDescriptor;\n",
    "import org.apache.beam.sdk.extensions.sql.SqlTransform;\n",
    "import org.joda.time.Instant;\n",
    "\n",
    "String args[] = new String[] {\"--tempLocation=gs://kolban-tmp\"};\n",
    "var options = PipelineOptionsFactory.fromArgs(args).withValidation().create();\n",
    "\n",
    "// Disable block on run for direct runner\n",
    "options.as(DirectOptions.class).setBlockOnRun(false);\n",
    "//options.as(org.apache.beam.sdk.extensions.sql.impl.BeamSqlPipelineOptions.class).setPlannerName(\"org.apache.beam.sdk.extensions.sql.zetasql.ZetaSQLQueryPlanner\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c029ab0",
   "metadata": {},
   "source": [
    "Before we can talk about Beam SQL, we need to understand the `Row` data type.  A Row is an element in a PCollection that represents the data that the SQL will work upon.  Consider a logical table called `Sale` that contains:\n",
    "\n",
    "* item: String - What was sold\n",
    "* amount: Double - How much was it sold for\n",
    "* time: Instant - When was it sold\n",
    "\n",
    "To create a Row that represents an instance of a `Sale`, we must first start by describing the *schema*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5416c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Schema saleSchema = Schema\n",
    "  .builder()\n",
    "  .addStringField(\"item\")\n",
    "  .addDoubleField(\"amount\")\n",
    "  .addDateTimeField(\"time\")\n",
    "  .build();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a6b61",
   "metadata": {},
   "source": [
    "The types of fields that can be added are:\n",
    "\n",
    "* BOOLEAN\n",
    "* BYTE\n",
    "* BYTES\n",
    "* DATETIME\n",
    "* DECIMAL\n",
    "* DOUBLE\n",
    "* FLOAT\n",
    "* INT16\n",
    "* INT32\n",
    "* INT64\n",
    "* STRING\n",
    "\n",
    "Now that we have a schema, we can create an instance of a Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868d5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Row row = Row.withSchema(saleSchema)\n",
    "  .withFieldValue(\"item\", \"blue\")\n",
    "  .withFieldValue(\"amount\", 10.12)\n",
    "  .withFieldValue(\"time\", Instant.now())\n",
    "  .build();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce515c",
   "metadata": {},
   "source": [
    "Let's put it together.  Here is a PoJo called Sale that includes the ability to get a Row from an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a444d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "public class Sale implements Serializable{\n",
    "  private static final Schema saleSchema = Schema\n",
    "    .builder()\n",
    "    .addStringField(\"item\")\n",
    "    .addDoubleField(\"amount\")\n",
    "    .addDateTimeField(\"time\")\n",
    "    .build();\n",
    "  private String item;\n",
    "  private Double amount;\n",
    "  private Instant time;\n",
    "  \n",
    "  public Sale(String item, Double amount, Instant time) {\n",
    "    this.item = item;\n",
    "    this.amount = amount;\n",
    "    this.time = time;\n",
    "  }\n",
    "  \n",
    "  public String getItem() { return item; }\n",
    "  public Double getAmount() { return amount; }\n",
    "  public Instant getTime() { return time; }\n",
    "  public String toString() {\n",
    "    return \"item: \" + item + \", amount: \" + amount + \", time: \" + time;\n",
    "  }\n",
    "  public Row getRow() {\n",
    "    Row row = Row.withSchema(saleSchema)\n",
    "      .withFieldValue(\"item\", item)\n",
    "      .withFieldValue(\"amount\", amount)\n",
    "      .withFieldValue(\"time\", time)\n",
    "      .build();\n",
    "    return row;\n",
    "  }\n",
    "  \n",
    "  public static final Schema getSchema() {\n",
    "    return saleSchema;\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2f344d",
   "metadata": {},
   "source": [
    "And now we can create a PCollection of Row for us to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c5d2e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: \n",
      "item:red\n",
      "amount:15.0\n",
      "time:2022-12-12T00:00:00.000Z\n",
      "\n",
      "Row: \n",
      "item:green\n",
      "amount:12.0\n",
      "time:2022-12-11T00:00:00.000Z\n",
      "\n",
      "Row: \n",
      "item:yellow\n",
      "amount:14.0\n",
      "time:2022-12-13T00:00:00.000Z\n",
      "\n",
      "Row: \n",
      "item:blue\n",
      "amount:10.0\n",
      "time:2022-12-11T00:00:00.000Z\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public class LoggingDoFn<T> extends DoFn<T, T>  {\n",
    "  @ProcessElement\n",
    "  public void processElement(@Element T element, OutputReceiver<T> out) {\n",
    "    System.out.println(element);\n",
    "    out.output(element);\n",
    "  }\n",
    "} // LoggingDoFn\n",
    "\n",
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create elements\", Create.of(\n",
    "    new Sale(\"blue\", 10.0, Instant.parse(\"2022-12-11\")).getRow(),\n",
    "    new Sale(\"green\", 12.0, Instant.parse(\"2022-12-11\")).getRow(),    \n",
    "    new Sale(\"red\", 15.0, Instant.parse(\"2022-12-12\")).getRow(),\n",
    "    new Sale(\"yellow\", 14.0, Instant.parse(\"2022-12-13\")).getRow()\n",
    "  ))\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "  \n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b91bbf",
   "metadata": {},
   "source": [
    "Now we can invoke a SQL transform.  Notice the call to setRowSchema().  We need to associate a schema with the PCollection before we can invoke the SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac9d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: \n",
      "total_sum:51.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create elements\", Create.of(\n",
    "    new Sale(\"blue\", 10.0, Instant.parse(\"2022-12-11\")).getRow(),\n",
    "    new Sale(\"green\", 12.0, Instant.parse(\"2022-12-11\")).getRow(),    \n",
    "    new Sale(\"red\", 15.0, Instant.parse(\"2022-12-12\")).getRow(),\n",
    "    new Sale(\"yellow\", 14.0, Instant.parse(\"2022-12-13\")).getRow()\n",
    "  ))\n",
    "  .setRowSchema(Sale.getSchema())\n",
    "  .apply(\"SQL\", SqlTransform.query(\"SELECT SUM(amount) as total_sum FROM PCOLLECTION\"))\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "  \n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a247dae",
   "metadata": {},
   "source": [
    "## Dataflow SQL\n",
    "Dataflow provides source of GCS, PubSub and BigQuery and sinks of PubSub and BigQuery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.16+8-post-Debian-1deb10u1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
