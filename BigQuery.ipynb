{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb7f722",
   "metadata": {},
   "source": [
    "# Apache Beam - BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a283caf",
   "metadata": {},
   "source": [
    "BigQuery is Google's cloud based Data Warehouse.  BigQuery can be used as a source or sink of data in a Beam pipeline.  In this notebook we will explore some of the BigQuery capabilities available in Beam.\n",
    "\n",
    "\n",
    "First, we define the dependencies that we wish to load from the Maven repositories.\n",
    "\n",
    "See also:\n",
    "* [Google BigQuery I/O connector](https://beam.apache.org/documentation/io/built-in/google-bigquery/)\n",
    "* [JavaDoc: BigQueryIO](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a7cef",
   "metadata": {},
   "source": [
    "Since our notebook is going to use Google Cloud SDK JARS we must include these in our dependencies.  Specifically, we need to include:\n",
    "\n",
    "```\n",
    "<dependency>\n",
    "  <groupId>org.apache.beam</groupId>\n",
    "  <artifactId>beam-sdks-java-io-google-cloud-platform</artifactId>\n",
    "  <version>2.43.0</version>\n",
    "</dependency>\n",
    "```\n",
    "\n",
    "Normally we would load our dependencies using the IJava Jupyter cell magic called `%%loadFromPom`.  Unfortunately, this doesn't work ([issue](https://github.com/SpencerPark/IJava/issues/139)).  A workaround is to download the dependencies outside of Jupyter and then launch Jupyter with the downloaded dependencies in the classpath.\n",
    "\n",
    "```\n",
    "mvn dependency:copy-dependencies\n",
    "export IJAVA_CLASSPATH=\"./target/dependency/*\"\n",
    "jupyter notebook\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211db4d",
   "metadata": {},
   "source": [
    "Next we define our imports required for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a00821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.options.Default;\n",
    "import org.apache.beam.sdk.options.Description;\n",
    "import org.apache.beam.sdk.options.PipelineOptionsFactory;\n",
    "import org.apache.beam.sdk.options.PipelineOptions;\n",
    "import org.apache.beam.sdk.options.StreamingOptions;\n",
    "import org.apache.beam.sdk.transforms.Create;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.coders.KvCoder;\n",
    "import org.apache.beam.sdk.coders.StringUtf8Coder;\n",
    "import org.apache.beam.sdk.values.KV;\n",
    "import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;\n",
    "import org.apache.beam.sdk.transforms.Sample;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.io.gcp.bigquery.SchemaAndRecord;\n",
    "import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.WriteDisposition;\n",
    "import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;\n",
    "import com.google.api.services.bigquery.model.TableSchema;\n",
    "import com.google.api.services.bigquery.model.TableFieldSchema;\n",
    "import com.google.api.services.bigquery.model.TableRow;\n",
    "import org.apache.beam.sdk.io.gcp.bigquery.TableRowJsonCoder;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.values.TypeDescriptor;\n",
    "import com.google.api.services.bigquery.model.TableReference;\n",
    "import org.apache.beam.sdk.io.gcp.bigquery.InsertRetryPolicy;\n",
    "\n",
    "String args[] = new String[] {\"--tempLocation=gs://kolban-tmp\"};\n",
    "var options = PipelineOptionsFactory.fromArgs(args).withValidation().create();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad43a0b9",
   "metadata": {},
   "source": [
    "## Querying and reading\n",
    "Let us now read a table.  You must first create the table that you want to read from.  We will use a Google provided sample that is reasonably small.  The table is called `bigquery-public-data.samples.shakespeare` and contains:\n",
    "\n",
    "* `word:STRING`\n",
    "* `word_count:INTEGER`\n",
    "* `corpus:STRING`\n",
    "* `corpus_date:INTEGER`\n",
    "\n",
    "The table contains 164K rows and appears to be about 6.13MB in size.\n",
    "\n",
    "In this first example, we read a table fully.  The return is a PCollection of `TableRow` where each element corresponds to a single row in the table.\n",
    "\n",
    "**Note**: The [TableRow](https://www.javadoc.io/static/com.google.apis/google-api-services-bigquery/v2-rev20220827-2.0.0/com/google/api/services/bigquery/model/TableRow.html) is a container meaning that it contains name/value pairs.  Each name corresponds to a column and the value to the cell in the table.  We can retrieve a value using the `get(name)` method:\n",
    "\n",
    "```\n",
    "TableRow row=...;\n",
    "String word = row.get(\"word\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843fa5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenericData{classInfo=[f], {word=LVII, word_count=1, corpus=sonnets, corpus_date=0}}\n",
      "GenericData{classInfo=[f], {word=augurs, word_count=1, corpus=sonnets, corpus_date=0}}\n",
      "GenericData{classInfo=[f], {word=dimm'd, word_count=1, corpus=sonnets, corpus_date=0}}\n",
      "GenericData{classInfo=[f], {word=plagues, word_count=1, corpus=sonnets, corpus_date=0}}\n",
      "GenericData{classInfo=[f], {word=treason, word_count=1, corpus=sonnets, corpus_date=0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public class LoggingDoFn<T> extends DoFn<T, T>  {\n",
    "  @ProcessElement\n",
    "  public void processElement(@Element T element, OutputReceiver<T> out) {\n",
    "    System.out.println(element);\n",
    "    out.output(element);\n",
    "  }\n",
    "}\n",
    "\n",
    "var tableName = \"bigquery-public-data:samples.shakespeare\";\n",
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Read table\", BigQueryIO.readTableRows().from(tableName))\n",
    "  .apply(\"Sample\", Sample.any(5))  // Just take 5 of the rows at random\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03fe12f",
   "metadata": {},
   "source": [
    "Instead of reading the whole table, we can execute a query.  In the following we'll return the most frequently encountered words.  Notice the requirement to add `usingStandardSql()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761d9a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenericData{classInfo=[f], {word=the, word_count=995, corpus=hamlet, corpus_date=1600}}\n",
      "GenericData{classInfo=[f], {word=the, word_count=942, corpus=coriolanus, corpus_date=1607}}\n",
      "GenericData{classInfo=[f], {word=the, word_count=937, corpus=kinghenryv, corpus_date=1599}}\n",
      "GenericData{classInfo=[f], {word=the, word_count=894, corpus=2kinghenryiv, corpus_date=1598}}\n",
      "GenericData{classInfo=[f], {word=the, word_count=848, corpus=kingrichardiii, corpus_date=1592}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Read table\", BigQueryIO\n",
    "    .readTableRows()\n",
    "    .fromQuery(\"select * FROM `bigquery-public-data.samples.shakespeare` order by word_count desc limit 5\")\n",
    "    .withQueryPriority(BigQueryIO.TypedRead.QueryPriority.INTERACTIVE)\n",
    "    .usingStandardSql())\n",
    "  .apply(\"Print elements\",ParDo.of(new LoggingDoFn<>()));\n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354b36c",
   "metadata": {},
   "source": [
    "There are multiple styles/methods that can be used to read data from BigQuery.  One is `EXPORT` where the table is exported to Cloud Storage in Avro and then read from Avro.  The other is `DIRECT_READ` where the BigQuery storage API is used.  In the following, we show us switching to `DIRECT_READ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64ec1ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenericData{classInfo=[f], {word=the, word_count=995, corpus=hamlet, corpus_date=1600}}\n",
      "GenericData{classInfo=[f], {word=the, word_count=942, corpus=coriolanus, corpus_date=1607}}\n",
      "GenericData{classInfo=[f], {word=the, word_count=937, corpus=kinghenryv, corpus_date=1599}}\n",
      "GenericData{classInfo=[f], {word=the, word_count=894, corpus=2kinghenryiv, corpus_date=1598}}\n",
      "GenericData{classInfo=[f], {word=the, word_count=848, corpus=kingrichardiii, corpus_date=1592}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Read table\", BigQueryIO\n",
    "    .readTableRows()\n",
    "    .fromQuery(\"select * FROM `bigquery-public-data.samples.shakespeare` order by word_count desc limit 5\")\n",
    "    .withQueryPriority(BigQueryIO.TypedRead.QueryPriority.INTERACTIVE)\n",
    "    .usingStandardSql()\n",
    "    .withMethod(BigQueryIO.TypedRead.Method.DIRECT_READ))\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7108954",
   "metadata": {},
   "source": [
    "We have looked at retrieving data as `TableRows` but maybe we want to work with our own data.  An alternative mechanism is to supply a function which is called for each row returned.  The function is passed a [SchemaAndRecord](https://beam.apache.org/releases/javadoc/2.42.0/org/apache/beam/sdk/io/gcp/bigquery/SchemaAndRecord.html) instance and returns your own record structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9baeafef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the:995\n",
      "the:942\n",
      "the:937\n",
      "the:894\n",
      "the:848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyFn implements SerializableFunction<SchemaAndRecord, String> {\n",
    "  public String apply(SchemaAndRecord input) {\n",
    "    var record = input.getRecord();\n",
    "    String result = record.get(\"word\").toString() + ':' + record.get(\"word_count\");\n",
    "    return result;\n",
    "  }\n",
    "}\n",
    "\n",
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Read table\", BigQueryIO\n",
    "    .read(new MyFn())\n",
    "    .fromQuery(\"select * FROM `bigquery-public-data.samples.shakespeare` order by word_count desc limit 5\")\n",
    "    .withQueryPriority(BigQueryIO.TypedRead.QueryPriority.INTERACTIVE)         \n",
    "    .usingStandardSql())\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbde244",
   "metadata": {},
   "source": [
    "There are many other reading options we could consider.\n",
    "\n",
    "* `withSelectedFields()` - If we are are reading the whole table without running a query, we can constrain which columns are returned.\n",
    "\n",
    "## Writing to BigQuery\n",
    "Now we turn our attention to writing to BigQuery.  When we write to BigQuery, either the target table exists or it does not.  We can control what we wish to do using the `withCreateDisposition()`.  Our choices are either:\n",
    "\n",
    "* `CREATE_NEVER` - Fail the request as the table should have existed.\n",
    "* `CREATE_IF_NEEDED` - Create the table if it doesn't already exist.  We must also specify the schema for the table that is to be created.\n",
    "\n",
    "Since we are adding rows to a table, the next question becomes what we want to happen if there are existing rows.  We can specify this with the `withWriteDisposition()` function.  Our choices are:\n",
    "\n",
    "* `WRITE_EMPTY` - Fail the request if the table is not empty.\n",
    "* `WRITE_TRUNCATE` - Empty the table before writing new rows.\n",
    "* `WRITE_APPEND` - Append the new rows into the table leaving the existing rows untouched.\n",
    "\n",
    "When the actual rows are to be inserted into a table, there are a few methods available to us.\n",
    "\n",
    "* load jobs (`FILE_LOADS`)\n",
    "* streaming inserts (`STREAMING_INSERTS`)\n",
    "* storage write (`STORAGE_WRITE_API`)\n",
    "\n",
    "We can select which one we wish to use with the `writeMethod()` function.  The default for a bounded PCollection is `FILE_LOADS` while for an unbounded PCollection it is `STREAMING_INSERTS`.\n",
    "\n",
    "In order to write to BigQuery, we need to think in terms of TableRow objects.  We can either pass in a `PCollection<TableRow>` or pass in a `PCollection<???>` and provide a `withFormatFunction`.  The `withFormatFunction` is a function that will be invoked for each element in the PCollection that must return a TableRow.  Basically, a converter to TableRow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1db34a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "String targetTableName = \"test1-305123:jupyter.test1\";\n",
    "\n",
    "TableSchema schema = new TableSchema().setFields(\n",
    "  Arrays.asList(\n",
    "    new TableFieldSchema().setName(\"name\").setType(\"STRING\"),\n",
    "    new TableFieldSchema().setName(\"salary\").setType(\"FLOAT64\"),\n",
    "    new TableFieldSchema().setName(\"tenure\").setType(\"INT64\")\n",
    "  )\n",
    ");\n",
    "\n",
    "class Employee implements Serializable {\n",
    "  private String name;\n",
    "  private Double salary;\n",
    "  private Integer tenure;\n",
    "  \n",
    "  public Employee(String name, Double salary, Integer tenure) {\n",
    "    this.name = name;\n",
    "    this.salary = salary;\n",
    "    this.tenure = tenure;\n",
    "  }\n",
    "  \n",
    "  public String getName() {\n",
    "    return name;\n",
    "  }\n",
    "  \n",
    "  public Double getSalary() {\n",
    "    return salary;\n",
    "  }\n",
    "  \n",
    "  public Integer getTenure() {\n",
    "    return tenure;\n",
    "  }\n",
    "  \n",
    "  public String toString() {\n",
    "    return \"name: \" + name + \", salary: \" + salary + \", tenure: \" + tenure;\n",
    "  }\n",
    "}\n",
    "\n",
    "class EmployeeToTableRow implements SerializableFunction<Employee, TableRow> {\n",
    "  public TableRow apply(Employee employee) {\n",
    "    TableRow r = new TableRow();\n",
    "    r.set(\"name\", employee.getName());\n",
    "    r.set(\"salary\", employee.getSalary());\n",
    "    r.set(\"tenure\", employee.getTenure());\n",
    "    return r;\n",
    "  }\n",
    "}\n",
    "\n",
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create Rows\", Create.\n",
    "    of(\n",
    "      new Employee(\"Neil\", 50000.11, 48),\n",
    "      new Employee(\"Sue\", 75000.99, 12),\n",
    "      new Employee(\"Bob\", 45000.32, 6)\n",
    "    )\n",
    "  )\n",
    "  .apply(\"Write table\", BigQueryIO\n",
    "    .<Employee>write()\n",
    "    .to(targetTableName)\n",
    "    .withFormatFunction(new EmployeeToTableRow())\n",
    "    .withSchema(schema)\n",
    "    .withTableDescription(\"My Test Table\")\n",
    "    .withMethod(BigQueryIO.Write.Method.STREAMING_INSERTS)\n",
    "    .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)\n",
    "    .withWriteDisposition(WriteDisposition.WRITE_APPEND));\n",
    "\n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de195f",
   "metadata": {},
   "source": [
    "There are many additional options that can be specified when we write to a BigQuery table:\n",
    "\n",
    "* `withTableDescription` - Provide a description of the table.\n",
    "* `withMethod` - How the rows are written to BigQuery.  Choices include FILE_LOADS, STORAGE_API_AT_LEAST_ONCE, STORAGE_WRITE_API, STREAMING_INSERTS\n",
    "* `withoutValidation` - Don't perform validation on the BigQuery command\n",
    "* `withLoadProjectId` - The Google Cloud project to be billed for the request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e64c4e",
   "metadata": {},
   "source": [
    "## Error Handling\n",
    "What if something goes wrong with our BigQuery call?  How can we catch this and take remedial action?\n",
    "\n",
    "The following pipeline fragment will fail.  It attemptes to insert rows into a table that doesn't exist and we have instructed the transform to *not* create the table.\n",
    "\n",
    "To solve the puzzle, we notice that a BigQueryIO.write transform returns an instance of [WriteResult](https://beam.apache.org/releases/javadoc/2.42.0/org/apache/beam/sdk/io/gcp/bigquery/WriteResult.html).  This is **not** a PCollection.  The WriteResult object contains methods to retrieve PCollections that have been stored within it which indicate the errors encountered:\n",
    "\n",
    "* `getFailedInserts(): PCollection<TableRow>`\n",
    "* `getFailedInsertsWithErr(): PCollection<BigQueryInsertError>`\n",
    "* `getFailedStorageApiInserts(): PCollection<BigQueryStorageApiInsertError>`\n",
    "* `getSuccessfulInserts(): PCollection<TableRow>` - **Warning**: This appears to only be available for STREAMING_INSERTS.\n",
    "* `getSuccessfulTableLoads(): PCollection<TableDestination>`\n",
    "\n",
    "To engage some of these results, we need to add some extract options to the transform:\n",
    "\n",
    "* `withExtendedErrorInfo()` is needed to have data available to `getFailedInsertsWithErr()`\n",
    "\n",
    "The transform can perform some error handling by itself.  We get to control this through `withFailedInsertRetryPolicy` which can take one of:\n",
    "\n",
    "* alwaysRetry\n",
    "* neverRetry\n",
    "* retryTransientErrors\n",
    "\n",
    "It also appears that there are distinctions between error handling and how the data is inserted.  For example, when we tried to load NaN it worked with FILE_LOADS and STORAGE_WRITE_API but failed with STREAMING_INSERTS.\n",
    "\n",
    "When using STREAMING_INSERTS the data will be converted into JSON for transmission.  This can result in some unexpected errors.  For example, sending a row with the values:\n",
    "\n",
    "```\n",
    "name=\"Bob\", salary=NaN, tenure=6\n",
    "```\n",
    "doesn't appear to be possible because we can't express NaN (Not a Number) in JSON.  However if we use the insertion method of `STORAGE_WRITE_API` or `FILE_LOADS` it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc97a909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to run pipeline:\n",
      "name: Neil, salary: 50000.11, tenure: 48\n",
      "name: null, salary: 3.2, tenure: 6\n",
      "name: Sue, salary: 75000.99, tenure: 12\n"
     ]
    },
    {
     "ename": "EvalException",
     "evalue": "java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: com.google.api.gax.rpc.PermissionDeniedException: io.grpc.StatusRuntimeException: PERMISSION_DENIED: Permission 'TABLES_UPDATE_DATA' denied on resource 'projects/test1-305123/datasets/jupyter/tables/not_here' (or it may not exist).",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1m\u001b[31morg.apache.beam.sdk.Pipeline$PipelineExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: com.google.api.gax.rpc.PermissionDeniedException: io.grpc.StatusRuntimeException: PERMISSION_DENIED: Permission 'TABLES_UPDATE_DATA' denied on resource 'projects/test1-305123/datasets/jupyter/tables/not_here' (or it may not exist).\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.beam.runners.direct.DirectRunner$DirectPipelineResult.waitUntilFinish(DirectRunner.java:374)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.beam.runners.direct.DirectRunner$DirectPipelineResult.waitUntilFinish(DirectRunner.java:342)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.beam.runners.direct.DirectRunner.run(DirectRunner.java:218)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.beam.runners.direct.DirectRunner.run(DirectRunner.java:67)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.beam.sdk.Pipeline.run(Pipeline.java:323)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.beam.sdk.Pipeline.run(Pipeline.java:309)\u001b[0m",
      "\u001b[1m\u001b[31m\tat .(#64:1)\u001b[0m"
     ]
    }
   ],
   "source": [
    "String targetTableName_notpresent = \"test1-305123:jupyter.not_here\";\n",
    "\n",
    "var pipeline = Pipeline.create(options);\n",
    "var writeResult = pipeline\n",
    "  .apply(\"Create Rows\", Create.<Employee>\n",
    "    of(\n",
    "      new Employee(\"Neil\", 50000.11, 48),\n",
    "      new Employee(\"Sue\", 75000.99, 12),\n",
    "      new Employee(null, 3.2, 6)\n",
    "    )\n",
    "  )\n",
    "  .apply(\"Print initial elements\", ParDo.of(new LoggingDoFn<>()))\n",
    "  .apply(\"Write table\", BigQueryIO\n",
    "    .<Employee>write()\n",
    "    .to(targetTableName_notpresent)\n",
    "    .withExtendedErrorInfo()\n",
    "    .withFormatFunction(new EmployeeToTableRow())\n",
    "    .withSchema(schema)\n",
    "    //.withFailedInsertRetryPolicy(InsertRetryPolicy.retryTransientErrors())\n",
    "    .withTableDescription(\"My Test Table\")\n",
    "    .withMethod(BigQueryIO.Write.Method.STORAGE_WRITE_API)\n",
    "    .withCreateDisposition(CreateDisposition.CREATE_NEVER)\n",
    "    .withWriteDisposition(WriteDisposition.WRITE_APPEND));\n",
    "\n",
    "//writeResult.getFailedStorageApiInserts()\n",
    "//  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "//writeResult.getSuccessfulInserts()\n",
    "//  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "\n",
    "System.out.println(\"About to run pipeline:\");\n",
    "pipeline.run().waitUntilFinish();\n",
    "System.out.println(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49885339",
   "metadata": {},
   "source": [
    "## Final Notes\n",
    "* It has been found that the IJava Jupyter package (1.3) was last built back in 2018.  This package builds a *fat jar* meaning that its dependencies are bundled into it.  One of those dependencies was com.google.gson which is a JSON parsing library from Google.  Unfortunately, the code from 2018 has less capability than the code for today and the Beam BigQueryIO depends on that code.  This resulted in some very strange errors that were hard to diagnose.  A workaround was found where we patched the IJava build with the latest GSON code base taken from Maven."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df289f5e",
   "metadata": {},
   "source": [
    "## References\n",
    "* [BigQueryIO Source on Github](https://github.com/apache/beam/tree/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0442de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.16+8-post-Debian-1deb10u1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
