{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb7f722",
   "metadata": {},
   "source": [
    "# Apache Beam - Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07552432",
   "metadata": {},
   "source": [
    "Windowing is the ability to group together input data into distinct collections of data based upon time.  Every element in a PCollection has an explicit or implicit time value associated with it.\n",
    "\n",
    "We have previously seen the concept of an unbounded PCollection.  This is a PCollection that has a potentially infinite number of elements.  Consider applying an unbounded PCollection to a transform such as GroupByKey or Combine.  These transforms work on complete PCollections.  For example, GroupByKey wants to examine *every* element in a PCollection and place it in a group.  How can that be done if there are always *more* elements that could arrive?  The answer is to perform the transform on a set of elements and not *all* the possible elements.  The way we build that set of elements is based on time.  For example, we could create a set of all elements that have arrived in the last hour.  This would be a bounded set of elements.\n",
    "\n",
    "The entire logical PCollection is thus split into multiple sets based on the concept of a window of time.\n",
    "\n",
    "There is always a default window called the Global Window.  This is the span of time from -ve Infinity to +ve Infinity (i.e. all time).\n",
    "\n",
    "Windowing also has concepts called:\n",
    "\n",
    "* Watermarks\n",
    "* Triggers\n",
    "* Panes\n",
    "* Late arrivals\n",
    "\n",
    "Windowing works with primitives that are windowing aware.  These include `GroupByKey` and `Combine`.\n",
    "\n",
    "When we think about data arriving as input into Beam, each element of data has an associated timestamp.  We call this the *event* time.  It is the time that the event is considered to have actually happened.  For example, consider a purchase in a store.  There was a moment in time where the purchase happened.  We may then end up with a record such as:\n",
    "\n",
    "```\n",
    "Sale(item=\"Blue\", amount=12.0, time=\"2022-12-21T13:30:01Z\")\n",
    "```\n",
    "\n",
    "Now consider this record being processed by beam.  It will be processed at some time in the future.  We call this the *processing* time.  We now have *two* concepts of time.  One being event time which is when the event is said to have happened and the other being processing time which is when we are processing the event.\n",
    "\n",
    "By considering these two notions of time, we can form some conclusions.  The first is that processing time always has to be later than event time for a given element.  Hopefully this is obvious.  We can't process an event before the event has happened.\n",
    "\n",
    "Our ideal situation is one where the difference between processing time and event time is as small as possible.  In English, we would say that the data is processed as soon after its creation as possible.  However, it is usually not the case that the this difference (which we call lag) is zero.  Instead, there will be a delay between the event occurring and the event being processed.  For example, if it takes 100ms for an event to transition across a network, then the processing time will be no less than 100ms after event time.\n",
    "\n",
    "Now imagine a magical function which we will call *watermark*.   This function takes as input an instant in event time and returns an instant in processing time such that the claim is that we will have received all possible events prior to this event time at the given processing time.  This becomes useful to us when we want to process groups of events.  Let us imagine sales records for our 24x7 online stores.  We want to aggregate all these sales records to produce a daily total.  A simple solution would be to wait till midnight (00:00:00) and then sum up all the records for the last 24 hours.  However, consider what we have just been discussing.  If we did our summation at exactly midnight, we would be missing records that have not yet been received but have actually happened.  If we assumed a latency of 5 minutes, then we shouldn't do our summation until at least 5 past midnight (00:05:00) or we are in danger of not including all the data.  Generically, the time we should start our processing is:\n",
    "\n",
    "```\n",
    "Time<processing> = watermark(Time<event>)\n",
    "```\n",
    "\n",
    "or for our example:\n",
    "\n",
    "```\n",
    "00:05:00 = watermark(00:00:00)\n",
    "```\n",
    "\n",
    "For further consideration, we can now classify messages into one of three categories.\n",
    "\n",
    "* A message received by beam < `watermark(Time<event>)` is considered *early*\n",
    "* A message received by beam = `watermark(Time<event>)` is considered *on time*\n",
    "* A message received by beam > `watermark(Time<event>)` is considered *late*\n",
    "\n",
    "If we are processing data that doesn't do any aggregations (eg. groupings or combines) then event time, processing time or late data doesn't come into play.  However, if we are doing aggregations, we need to consider the concepts of event time, processing time and late data.\n",
    "\n",
    "See also:\n",
    "* [Programming Guide: Windowing](https://beam.apache.org/documentation/programming-guide/#windowing)\n",
    "* [JavaDoc: Window](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/transforms/windowing/Window.html)\n",
    "* [JavaDoc: FixedWindows](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/transforms/windowing/FixedWindows.html)\n",
    "* [Video: How to process stream data on Apache Beam](https://www.youtube.com/watch?v=oJ-LueBvOcM)\n",
    "* [Video: Understanding exactly-once processing and windowing in streaming pipelines](https://www.youtube.com/watch?v=DraQGkARegE)\n",
    "* [Testing Unbounded Pipelines in Apache Beam](https://beam.apache.org/blog/test-stream/)\n",
    "* [Streaming 101: The world beyond batch](https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/)\n",
    "* [Streaming 102: The world beyond batch](https://www.oreilly.com/radar/the-world-beyond-batch-streaming-102/)\n",
    "* [Book: Streaming Systems](http://streamingsystems.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38402951",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%loadFromPOM\n",
    "\n",
    "<dependency>\n",
    "  <groupId>org.apache.beam</groupId>\n",
    "  <artifactId>beam-sdks-java-core</artifactId>\n",
    "  <version>2.43.0</version>\n",
    "</dependency>\n",
    "\n",
    "<dependency>\n",
    "  <groupId>org.apache.beam</groupId>\n",
    "  <artifactId>beam-runners-direct-java</artifactId>\n",
    "  <version>2.43.0</version>\n",
    "  <scope>runtime</scope>\n",
    "</dependency>\n",
    "\n",
    "<dependency>\n",
    "  <groupId>org.slf4j</groupId>\n",
    "  <artifactId>slf4j-api</artifactId>\n",
    "  <version>2.0.6</version>\n",
    "</dependency>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211db4d",
   "metadata": {},
   "source": [
    "Next we define our imports required for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a00821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.options.Default;\n",
    "import org.apache.beam.sdk.options.Description;\n",
    "import org.apache.beam.sdk.options.PipelineOptionsFactory;\n",
    "import org.apache.beam.sdk.options.PipelineOptions;\n",
    "import org.apache.beam.sdk.options.StreamingOptions;\n",
    "import org.apache.beam.sdk.transforms.Create;\n",
    "import org.apache.beam.sdk.transforms.GroupByKey;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.join.CoGbkResult;\n",
    "import org.apache.beam.sdk.transforms.join.KeyedPCollectionTuple;\n",
    "import org.apache.beam.sdk.transforms.join.CoGroupByKey;\n",
    "import org.apache.beam.sdk.transforms.Combine.CombineFn;\n",
    "import org.apache.beam.sdk.transforms.Combine;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.transforms.Sum;\n",
    "import org.apache.beam.sdk.transforms.Flatten;\n",
    "import org.apache.beam.sdk.transforms.windowing.Window;\n",
    "import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n",
    "import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n",
    "import org.apache.beam.sdk.transforms.Count;\n",
    "import org.apache.beam.sdk.transforms.WithTimestamps;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.PCollectionList;\n",
    "import org.apache.beam.sdk.values.KV;\n",
    "import org.apache.beam.sdk.values.PDone;\n",
    "import org.apache.beam.sdk.values.TupleTag;\n",
    "import org.apache.beam.sdk.values.TypeDescriptor;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.coders.KvCoder;\n",
    "import org.apache.beam.sdk.coders.StringUtf8Coder;\n",
    "import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n",
    "import org.apache.beam.sdk.schemas.JavaBeanSchema;\n",
    "\n",
    "//import java.time.Instant;\n",
    "import org.joda.time.Instant;\n",
    "import org.joda.time.Duration;\n",
    "\n",
    "\n",
    "String args[] = new String[] {};\n",
    "var options = PipelineOptionsFactory.fromArgs(args).withValidation().create();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7108954",
   "metadata": {},
   "source": [
    "Lets look at time stamps on elements.  Each element has a timestamp.  In a ParDo, we can access (and log) the timestamp using the `ProcessContext.getTime()`.  This returns an `org.joda.time.Instant` object.  Should we wish, we can also set the value of the timestamp on an element by calling `OutputReceiver.outputWithTimestamp`.  This could be useful if we have a `PCollection` of elements where the timestamp value is a property of the element data and not implicit in the element itself.  However, a better was is to use the pre-build transform called `WithTimestamps`.  This transform takes a `SerializableFunction` which receives elements as input and returns `Instant` (timestamp) values that are then associated with the elements.\n",
    "\n",
    "Some of the transforms that are source IO produce timestamps implicitly.  For example:\n",
    "\n",
    "* `PubsubIO` using `.withTimestampAttribute(...)` - Set the event timestamp from a named attribute.\n",
    "\n",
    "See also:\n",
    "* [JavaDoc: Class WithTimestamps](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/transforms/WithTimestamps.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b53365e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item: blue, amount: 10.0, time: 2022-12-11T00:00:00.000Z, timestamp:2022-12-11T00:00:00.000Z, pane: PaneInfo.NO_FIRING\n",
      "item: red, amount: 15.0, time: 2022-12-12T00:00:00.000Z, timestamp:2022-12-12T00:00:00.000Z, pane: PaneInfo.NO_FIRING\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    " * Execute a ParDo over the input PCollection<Sale> and for each of the Sale elements, set\n",
    " * the timestamp from the time property of the sale itself.\n",
    " */\n",
    "public class Sale implements Serializable{\n",
    "  private String item;\n",
    "  private Double amount;\n",
    "  private Instant time;\n",
    "  \n",
    "  public Sale(String item, Double amount, Instant time) {\n",
    "    this.item = item;\n",
    "    this.amount = amount;\n",
    "    this.time = time;\n",
    "  }\n",
    "  \n",
    "  public String getItem() { return item; }\n",
    "  public Double getAmount() { return amount; }\n",
    "  public Instant getTime() { return time; }\n",
    "  public String toString() {\n",
    "    return \"item: \" + item + \", amount: \" + amount + \", time: \" + time;\n",
    "  }\n",
    "} // Sale\n",
    "\n",
    "public class LoggingDoFn<T> extends DoFn<T, T>  {\n",
    "  @ProcessElement\n",
    "  public void processElement(@Element T element, OutputReceiver<T> out, ProcessContext context) {\n",
    "    System.out.println(element + \", timestamp:\" + context.timestamp() + \", pane: \" + context.pane());\n",
    "    out.output(element);\n",
    "  }\n",
    "} // LoggingDoFn\n",
    "\n",
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  // Create the elements\n",
    "  .apply(\"Create elements\", Create.of(\n",
    "    new Sale(\"blue\", 10.0, Instant.parse(\"2022-12-11\")),\n",
    "    new Sale(\"red\", 15.0, Instant.parse(\"2022-12-12\"))\n",
    "  ))\n",
    "  \n",
    "  /*\n",
    "  // Set the timestamps on the PCollection elements from the sale.time field.\n",
    "  .apply(WithTimestamps.of(new SerializableFunction<Sale, Instant>() {\n",
    "    public Instant apply(Sale sale) {\n",
    "      return sale.getTime();\n",
    "    }\n",
    "  }))\n",
    "  */\n",
    "  \n",
    "  // Here we set the timestamps of the elements using a lambda function.\n",
    "  .apply(WithTimestamps.of(sale -> sale.getTime()))\n",
    "  \n",
    "  // Log the elements to the output\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "  \n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f0fe6",
   "metadata": {},
   "source": [
    "Let us set ourselves a puzzle to count the value of sales by day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de910d66",
   "metadata": {},
   "outputs": [
    {
     "ename": "EvalException",
     "evalue": "Default values are not supported in Combine.globally() if the input PCollection is not windowed by GlobalWindows. Instead, use Combine.globally().withoutDefaults() to output an empty PCollection if the input PCollection is empty, or Combine.globally().asSingletonView() to get the default output of the CombineFn if the input PCollection is empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1m\u001b[31mjava.lang.IllegalStateException: Default values are not supported in Combine.globally() if the input PCollection is not windowed by GlobalWindows. Instead, use Combine.globally().withoutDefaults() to output an empty PCollection if the input PCollection is empty, or Combine.globally().asSingletonView() to get the default output of the CombineFn if the input PCollection is empty.\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.beam.sdk.transforms.Combine$Globally.expand(Combine.java:1192)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.beam.sdk.transforms.Combine$Globally.expand(Combine.java:1069)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.beam.sdk.Pipeline.applyInternal(Pipeline.java:548)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.beam.sdk.Pipeline.applyTransform(Pipeline.java:499)\u001b[0m",
      "\u001b[1m\u001b[31m\tat org.apache.beam.sdk.values.PCollection.apply(PCollection.java:373)\u001b[0m",
      "\u001b[1m\u001b[31m\tat .(#119:1)\u001b[0m"
     ]
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create elements\", Create.of(\n",
    "    new Sale(\"blue\",   10.0, Instant.parse(\"2022-12-11\")),\n",
    "    new Sale(\"green\",  10.0, Instant.parse(\"2022-12-11\")),    \n",
    "    new Sale(\"red\",    15.0, Instant.parse(\"2022-12-12\")),\n",
    "    new Sale(\"yellow\", 15.0, Instant.parse(\"2022-12-13\"))\n",
    "  )) \n",
    "  .apply(\"Set timestamps\", ParDo.of(new SaleTimestampDoFn()))\n",
    "  \n",
    "  .apply(\"Window\", Window\n",
    "    .<Sale>into(FixedWindows.of(Duration.standardDays(1)))\n",
    "    .triggering(AfterWatermark.pastEndOfWindow())\n",
    "    .withAllowedLateness(Duration.ZERO)\n",
    "    .discardingFiredPanes()\n",
    "  )\n",
    "  \n",
    "  \n",
    "  .apply(\"Get Amounts\", MapElements.into(TypeDescriptors.doubles()).via(new SerializableFunction<Sale, Double>(){\n",
    "    public Double apply(Sale sale) {\n",
    "      return sale.getAmount();\n",
    "    }\n",
    "  }))\n",
    "  .apply(\"Sum\", Sum.doublesGlobally())\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "  \n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0da74f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.16+8-post-Debian-1deb10u1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
