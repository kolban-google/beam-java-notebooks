{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb7f722",
   "metadata": {},
   "source": [
    "# Apache Beam - Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a283caf",
   "metadata": {},
   "source": [
    "A schema is meta data used to describe the structure of elements in a `PCollection`.  The schema is *attached* to the `PCollection`.\n",
    "\n",
    "A schema is defined as a set of named fields where each field can have a type.  The types of fields that can be used are:\n",
    "\n",
    "* BOOLEAN\n",
    "* BYTE\n",
    "* BYTES\n",
    "* DATETIME\n",
    "* DECIMAL\n",
    "* DOUBLE\n",
    "* FLOAT\n",
    "* INT16\n",
    "* INT32\n",
    "* INT64\n",
    "* STRING\n",
    "\n",
    "In addition, a field can be defined as a ROW which means that the field represents a nested schema.  Fields can also be tagged as:\n",
    "\n",
    "* ARRAY - The field can contain 0 or more values.\n",
    "* ITERABLE - The field can contain 0 or more values which can be iterated over.\n",
    "* MAP - The field contains name/value pairs.\n",
    "\n",
    "The schema becomes useful when the content of the `PCollection` are objects (records).  Imagine I told you that a `PCollection` contains Employee records.  Now consider if I told you that an Employee record contains:\n",
    "\n",
    "* name: String\n",
    "* salary: Double\n",
    "* tenure: Integer\n",
    "\n",
    "By declaring a schema, we are implicitly declaring that we have a mechanism for getting and setting values from an object.  With the ability to obtain a named value, we have now opened up a new capability in our data processing.  We can now write transforms that are cognitive of schemas.  Imagine we built a transform that takes as input the name of a field and a function that takes a value and returns true or false.  We could now imagine something like:\n",
    "\n",
    "```\n",
    "newPCollection<Employee> = originalPCollection.apply(Filter.on(\"salary\", Double d -> d > 10000))\n",
    "```\n",
    "\n",
    "We are now getting an inkling that by having Beam known the logical structure of elements, we can build new transforms that take advantage of it.\n",
    "\n",
    "Schemas can be defined explicitly or can be inferred.  We can infer a schema using the `@DefaultSchema` annotation.\n",
    "\n",
    "If a `PCollection` has a schema we can get its definition by calling `getSchema()`.  Not all `PCollections` have a schema.  We can determine if a `PCollection` has a chema by calling `hasSchema()`.\n",
    "\n",
    "Beam provides a class called `Row` that can be used to represent an element in a `PCollection` with an associated schema without having to create a corresponding Java Bean.\n",
    "\n",
    "We can create a `Row` using:\n",
    "\n",
    "```\n",
    "Row myRow = Row.schema(mySchema).withFieldValue(\"field1Name\", field1Value)....\n",
    "```\n",
    "\n",
    "The resulting row is immutable (can't change its values) but does have getters to obtain fields by name or ordinal.\n",
    "\n",
    "Q: What is `SchemaProvider`?\n",
    "Q: What is `SchemaRegistry`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a7cef",
   "metadata": {},
   "source": [
    "Since our notebook is going to use Google Cloud SDK JARS we must include these in our dependencies.  Specifically, we need to include:\n",
    "\n",
    "```\n",
    "<dependency>\n",
    "  <groupId>org.apache.beam</groupId>\n",
    "  <artifactId>beam-sdks-java-io-google-cloud-platform</artifactId>\n",
    "  <version>2.43.0</version>\n",
    "</dependency>\n",
    "```\n",
    "\n",
    "Normally we would load our dependencies using the IJava Jupyter cell magic called `%%loadFromPom`.  Unfortunately, this doesn't work ([issue](https://github.com/SpencerPark/IJava/issues/139)).  A workaround is to download the dependencies outside of Jupyter and then launch Jupyter with the downloaded dependencies in the classpath.\n",
    "\n",
    "```\n",
    "mvn dependency:copy-dependencies\n",
    "export IJAVA_CLASSPATH=\"./target/dependency/*\"\n",
    "jupyter notebook\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "See also:\n",
    "* [Programming Guide - Schemas](https://beam.apache.org/documentation/programming-guide/#schemas)\n",
    "* [Schema Patterns](https://beam.apache.org/documentation/patterns/schema/)\n",
    "* [JavaDoc: Row](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/values/Row.html)\n",
    "* [JavaDoc: Schema](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/schemas/Schema.html)\n",
    "* [JavaDoc: SqlTransform](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/extensions/sql/SqlTransform.html)\n",
    "* [Video: Schema-aware PCollections and Beam SQL (Beam Summit Europe 2019)](https://www.youtube.com/watch?v=aRIZXtQiCHw)\n",
    "* [Schema Aware Generic Pipelines Using Apache Beam Rows](https://medium.com/@gauravmishra_82578/schema-aware-generic-pipelines-using-apache-beam-rows-e0a36b11b929)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211db4d",
   "metadata": {},
   "source": [
    "Next we define our imports required for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a00821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.options.Default;\n",
    "import org.apache.beam.sdk.options.Description;\n",
    "import org.apache.beam.sdk.options.PipelineOptionsFactory;\n",
    "import org.apache.beam.sdk.options.PipelineOptions;\n",
    "import org.apache.beam.sdk.options.StreamingOptions;\n",
    "import org.apache.beam.sdk.transforms.Create;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.coders.KvCoder;\n",
    "import org.apache.beam.sdk.coders.StringUtf8Coder;\n",
    "import org.apache.beam.sdk.values.KV;\n",
    "import org.apache.beam.sdk.transforms.Sample;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.values.TypeDescriptor;\n",
    "import org.apache.beam.sdk.PipelineResult;\n",
    "import org.apache.beam.runners.direct.DirectOptions;\n",
    "import org.apache.beam.sdk.schemas.Schema;\n",
    "import org.apache.beam.sdk.values.Row;\n",
    "import org.apache.beam.sdk.extensions.sql.SqlTransform;\n",
    "import org.joda.time.Instant;\n",
    "import org.apache.beam.sdk.schemas.annotations.SchemaCreate;\n",
    "import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n",
    "import org.apache.beam.sdk.schemas.JavaBeanSchema;\n",
    "import org.apache.beam.sdk.schemas.transforms.Select;\n",
    "import org.apache.beam.sdk.schemas.transforms.Filter;\n",
    "import org.apache.beam.sdk.schemas.transforms.DropFields;\n",
    "import org.apache.beam.sdk.schemas.transforms.RenameFields;\n",
    "import org.apache.beam.sdk.schemas.transforms.Convert;\n",
    "import org.apache.beam.sdk.schemas.transforms.AddFields;\n",
    "import org.apache.beam.sdk.schemas.transforms.Group;\n",
    "import org.apache.beam.sdk.transforms.JsonToRow;\n",
    "import org.apache.beam.sdk.transforms.ToJson;\n",
    "\n",
    "String args[] = new String[] {\"--tempLocation=gs://kolban-tmp\"};\n",
    "var options = PipelineOptionsFactory.fromArgs(args).withValidation().create();\n",
    "\n",
    "// Disable block on run for direct runner\n",
    "options.as(DirectOptions.class).setBlockOnRun(false);\n",
    "\n",
    "public class LoggingDoFn<T> extends DoFn<T, T>  {\n",
    "  @ProcessElement\n",
    "  public void processElement(@Element T element, OutputReceiver<T> out) {\n",
    "    System.out.println(element);\n",
    "    out.output(element);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3ebe5",
   "metadata": {},
   "source": [
    "One way to define a schema is by using Java Beans.  Each of the properties of the bean (as defined by introspection to determine the getters and setters) becomes a field in the schema.  To define a schema from a Java Bean, annotate its declaration with `@DefaultSchema(JavaBeanSchema.class)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821d09d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@DefaultSchema(JavaBeanSchema.class)\n",
    "public class Employee {\n",
    "  private String name;\n",
    "  private Double salary;\n",
    "\n",
    "  public  String getName() { return name;}\n",
    "  public  void   setName(String name) { this.name = name; }\n",
    "  public  Double getSalary() { return salary; }\n",
    "  public  void   setSalary(Double salary) { this.salary = salary; }\n",
    "\n",
    "  public  String toString() {\n",
    "    return \"Employee: name: \" + name + \", salary: \" + salary;\n",
    "  }\n",
    "\n",
    "  @SchemaCreate\n",
    "  public Employee(String name, Double salary) {\n",
    "    this.name = name; this.salary = salary;\n",
    "  }\n",
    "} // Employee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c029ab0",
   "metadata": {},
   "source": [
    "Another way to create a schema is to explicitly construct one using the `Schema` class and the `builder()` method.  Invoking Schema.builder() returns a Schema.Builder object that allows us to add fields to our schema.  The order of insertion is remembered so that we can refer to the fields ordinally as well as by name.  We can add fields using the convenience constructors of the form `add<Type>Field(<fieldName>)` ... for example `addStringField(\"item\")` but if using this form, we don't get to specify non default values for things such as `description` or options.  Instead, we can create the field using `Schema.Field.of(...)` and add the field to the schema using `addFields(...)`.\n",
    "\n",
    "* [JavaDoc: Schema](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/schemas/Schema.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5416c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields:\n",
      "Field{name=item, description=, type=STRING NOT NULL, options={{}}}\n",
      "Field{name=amount, description=, type=DOUBLE NOT NULL, options={{}}}\n",
      "Field{name=time, description=, type=DATETIME NOT NULL, options={{}}}\n",
      "Encoding positions:\n",
      "{item=0, amount=1, time=2}\n",
      "Options:{{}}UUID: null\n"
     ]
    }
   ],
   "source": [
    "Schema saleSchema = Schema\n",
    "  .builder()\n",
    "  .addStringField(\"item\")\n",
    "  .addDoubleField(\"amount\")\n",
    "  .addDateTimeField(\"time\")\n",
    "  .build();\n",
    "System.out.println(saleSchema);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f871970",
   "metadata": {},
   "source": [
    "And here is an eequivalent using explicit field definitions which allows us to add a description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "718a9b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields:\n",
      "Field{name=item, description=Purchased Item, type=STRING NOT NULL, options={{}}}\n",
      "Field{name=amount, description=Purchased Cost, type=DOUBLE NOT NULL, options={{}}}\n",
      "Field{name=time, description=Purchased Time, type=DATETIME NOT NULL, options={{}}}\n",
      "Encoding positions:\n",
      "{item=0, amount=1, time=2}\n",
      "Options:{{}}UUID: null\n"
     ]
    }
   ],
   "source": [
    "Schema saleSchema = Schema\n",
    "  .builder()\n",
    "  .addFields(Schema.Field.of(\"item\",   Schema.FieldType.STRING).withDescription(\"Purchased Item\"))\n",
    "  .addFields(Schema.Field.of(\"amount\", Schema.FieldType.DOUBLE).withDescription(\"Purchased Cost\"))\n",
    "  .addFields(Schema.Field.of(\"time\",   Schema.FieldType.DATETIME).withDescription(\"Purchased Time\"))\n",
    "  .build();\n",
    "System.out.println(saleSchema);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b91bbf",
   "metadata": {},
   "source": [
    "Now let's look at some PTransforms that are schema aware.\n",
    "\n",
    "## Select\n",
    "\n",
    "First we have `Select`.  This transforms keeps (selects) only the named fields from the input.  This transform will return a `PCollection<Row>` where the `Row` contains only the named fields from the source.\n",
    "\n",
    "* [JavaDoc: Select](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/schemas/transforms/Select.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac9d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: \n",
      "name:Bob\n",
      "\n",
      "Row: \n",
      "name:Neil\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create elements\", Create.of(\n",
    "    new Employee(\"Neil\", 10000.00),\n",
    "    new Employee(\"Bob\",  20000.00)\n",
    "  ))\n",
    "  .apply(\"Select names\", Select.fieldNames(\"name\"))\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "  \n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506b6ae4",
   "metadata": {},
   "source": [
    "## DropFields\n",
    "An inverse of `Select` is called `DropFields`.  This transform drops the named fields from the input `PCollection`.  The output of the transform is a `PCollection<Row>`.  It can't be anything else since we have created data that doesn't map to a class.\n",
    "\n",
    "* [JavaDoc: DropFields](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/schemas/transforms/DropFields.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "795e089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: \n",
      "salary:20000.0\n",
      "\n",
      "Row: \n",
      "salary:10000.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create elements\", Create.of(\n",
    "    new Employee(\"Neil\", 10000.00),\n",
    "    new Employee(\"Bob\",  20000.00)\n",
    "  ))\n",
    "  .apply(\"Drop names\", DropFields.fields(\"name\"))\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "  \n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a40d32",
   "metadata": {},
   "source": [
    "## RenameFields\n",
    "We can rename fields using `RenameFields`.  The output of the transform is a `PCollection<Row>`.  It can't be anything else since we have created data that doesn't map to a class.\n",
    "\n",
    "* [JavaDoc: RenameFields](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/schemas/transforms/RenameFields.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c989446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: \n",
      "name:Bob\n",
      "pay:20000.0\n",
      "\n",
      "Row: \n",
      "name:Neil\n",
      "pay:10000.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create elements\", Create.of(\n",
    "    new Employee(\"Neil\", 10000.00),\n",
    "    new Employee(\"Bob\",  20000.00)\n",
    "  ))\n",
    "  .apply(\"Rename salary\", RenameFields.<Employee>create().rename(\"salary\", \"pay\"))\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "  \n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27154650",
   "metadata": {},
   "source": [
    "## Filter\n",
    "Next we will look at `Filter`.  This transform applies a predicate to each element and only the elements that evaluate to true are included in the output PCollection.  In the following example, we filter (keep) only elements that have a salary greater than 10000.  This transform returns a `PCollection<T>` matching the input type.\n",
    "\n",
    "* [JavaDoc: Filter](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/schemas/transforms/Filter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1900acca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee: name: Bob, salary: 20000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create elements\", Create.of(\n",
    "    new Employee(\"Neil\", 10000.00),\n",
    "    new Employee(\"Bob\",  20000.00)\n",
    "  ))\n",
    "  .apply(\"Filter salaries\", Filter.<Employee>create().whereFieldName(\"salary\",\n",
    "    new SerializableFunction<Double, Boolean>() {\n",
    "      public Boolean apply(Double amount) {\n",
    "        return amount > 10000.0;\n",
    "      }\n",
    "    }))\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "  \n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c93a5",
   "metadata": {},
   "source": [
    "## Convert\n",
    "We can convert from one Java object to another as long as they have identical schema.\n",
    "\n",
    "* [JavaDoc: Convert](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/schemas/transforms/Convert.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9676be91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: \n",
      "name:Neil\n",
      "salary:10000.0\n",
      "\n",
      "Row: \n",
      "name:Bob\n",
      "salary:20000.0\n",
      "\n",
      "Employee: name: Neil, salary: 10000.0\n",
      "Employee: name: Bob, salary: 20000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create elements\", Create.of(\n",
    "    new Employee(\"Neil\", 10000.00),\n",
    "    new Employee(\"Bob\",  20000.00)\n",
    "  ))\n",
    "  .apply(\"Convert to rows\", Convert.toRows())\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()))\n",
    "  .apply(\"Convert from rows\", Convert.fromRows(Employee.class))\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "  \n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcbdcdc",
   "metadata": {},
   "source": [
    "## AddFields\n",
    "We can add fields to a schema using AddFields.\n",
    "\n",
    "* [JavaDoc: AddFields](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/schemas/transforms/AddFields.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20d2dac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: \n",
      "name:Neil\n",
      "salary:10000.0\n",
      "isManager:false\n",
      "\n",
      "Row: \n",
      "name:Bob\n",
      "salary:20000.0\n",
      "isManager:false\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create elements\", Create.of(\n",
    "    new Employee(\"Neil\", 10000.00),\n",
    "    new Employee(\"Bob\",  20000.00)\n",
    "  ))\n",
    "  .apply(\"Add fields\", AddFields.<Employee>create().field(\"isManager\", Schema.FieldType.BOOLEAN, Boolean.FALSE))\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "  \n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c732bb",
   "metadata": {},
   "source": [
    "## Group\n",
    "We can group together elements by field using the `Group` transform.  The output of the transform is a `PCollection<Row>` but we need to take a few moments to discuss the schema of the resulting `Row`.\n",
    "\n",
    "The Row has schema:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"key\": {\n",
    "    \"type\": \"ROW\"\n",
    "    \"row\": {\n",
    "      \"<field1>\": \"<Field1Type>\"\n",
    "    }\n",
    "  },\n",
    "  \"value\": {\n",
    "    \"type\": \"ITERABLE,ROW\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This transform also provides for aggregation including global aggregation.\n",
    "\n",
    "* [JavaDoc: Group](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/schemas/transforms/Group.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89a83990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: \n",
      "key:Row: \n",
      "name:Bob\n",
      "\n",
      "value:[Row: \n",
      "name:Bob\n",
      "salary:20000.0\n",
      ", ]\n",
      "\n",
      "Row: \n",
      "key:Row: \n",
      "name:Neil\n",
      "\n",
      "value:[Row: \n",
      "name:Neil\n",
      "salary:10000.0\n",
      ", Row: \n",
      "name:Neil\n",
      "salary:15000.0\n",
      ", ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create elements\", Create.of(\n",
    "    new Employee(\"Neil\", 10000.00),\n",
    "    new Employee(\"Bob\",  20000.00),\n",
    "    new Employee(\"Neil\", 15000.00)\n",
    "  ))\n",
    "  .apply(\"Group\", Group.byFieldNames(\"name\"))\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3be513",
   "metadata": {},
   "source": [
    "## CoGroup\n",
    "* [JavaDoc: CoGroup](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/schemas/transforms/CoGroup.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bab9900",
   "metadata": {},
   "source": [
    "## Join\n",
    "* [JavaDoc: Join](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/schemas/transforms/Join.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be9cee",
   "metadata": {},
   "source": [
    "## SqlTransform\n",
    "There is a separate notebook dedicated to Beam SQL but this is also a good place to illustrate that we don't *have* to input a `PCollection<Row>`.  Any PCollection which has an attached schema can be used as input to the `SqlTransform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf2ab28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: \n",
      "total_salary:45000.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create elements\", Create.of(\n",
    "    new Employee(\"Neil\", 10000.00),\n",
    "    new Employee(\"Bob\",  20000.00),\n",
    "    new Employee(\"Neil\", 15000.00)\n",
    "  ))\n",
    "  .apply(\"SQL\", SqlTransform.query(\"SELECT SUM(salary) as total_salary FROM PCOLLECTION\"))\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cf3c7",
   "metadata": {},
   "source": [
    "## JsonToRow\n",
    "Through the use of schema, we can parse JSON strings.  To use, we define a schema and use the `JsonToRow.withSchema(schema)` method.  This takes as input a `PCollection<String>` and returns a `PCollection<Row>`.\n",
    "\n",
    "* [JavaDoc: JsonToRow](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/transforms/JsonToRow.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea7f48c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: \n",
      "item:blue\n",
      "amount:1.23\n",
      "time:2022-12-21T00:00:00.000Z\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create elements\", Create.of(\"{\\\"item\\\": \\\"blue\\\", \\\"amount\\\": 1.23, \\\"time\\\": \\\"2022-12-21\\\"}\"))\n",
    "  .apply(\"JSON\", JsonToRow.withSchema(saleSchema))\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9cf8ea",
   "metadata": {},
   "source": [
    "## ToJson\n",
    "We can also convert a `PCollection` which has a schema to a `PCollection<String>` of JSON encodings using the `ToJson` transform.\n",
    "\n",
    "* [JavaDoc: ToJson](https://beam.apache.org/releases/javadoc/2.43.0/org/apache/beam/sdk/transforms/ToJson.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f9c3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Neil\",\"salary\":15000.0}\n",
      "{\"name\":\"Bob\",\"salary\":20000.0}\n",
      "{\"name\":\"Neil\",\"salary\":10000.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DONE"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var pipeline = Pipeline.create(options);\n",
    "pipeline\n",
    "  .apply(\"Create elements\", Create.of(\n",
    "    new Employee(\"Neil\", 10000.00),\n",
    "    new Employee(\"Bob\",  20000.00),\n",
    "    new Employee(\"Neil\", 15000.00)\n",
    "  ))\n",
    "  .apply(\"To JSON\", ToJson.of())\n",
    "  .apply(\"Print elements\", ParDo.of(new LoggingDoFn<>()));\n",
    "pipeline.run().waitUntilFinish();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f763919b",
   "metadata": {},
   "source": [
    "## Schema Options\n",
    "So far we have seen that a Beam schema describes the meta data of a record.  That meta data is used to describe the *structure* of the record.  Specifically, it defines the fields and their types.  However, we may wish to have other meta data associated with a record.  Examples include:\n",
    "\n",
    "* Description\n",
    "* Lineage\n",
    "* Creation time\n",
    "* Allowed values for fields (data quality)\n",
    "* ... many more\n",
    "\n",
    "To accomodate this, the Beam schema allows us to associate arbitrary *options* with the schema.  An option is defined as a name, type and value.\n",
    "\n",
    "To obtain the options associated with a schema, we can call the getOptions() method.  This returns a Schema.Options object.\n",
    "\n",
    "See also:\n",
    "* [Video: Beam Schema Options](https://www.youtube.com/watch?v=Oi946DJVE7g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "672c8109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields:\n",
      "Field{name=item, description=Purchased Item, type=STRING NOT NULL, options={{myFieldOption=Option{type=STRING NOT NULL, value=Hello}}}}\n",
      "Field{name=amount, description=, type=DOUBLE NOT NULL, options={{}}}\n",
      "Field{name=time, description=, type=DATETIME NOT NULL, options={{}}}\n",
      "Encoding positions:\n",
      "{item=0, amount=1, time=2}\n",
      "Options:{{mySchemaOption=Option{type=STRING NOT NULL, value=Hello}}}UUID: null\n"
     ]
    }
   ],
   "source": [
    "Schema saleSchema = Schema\n",
    "  .builder()\n",
    "  .addFields(Schema.Field\n",
    "    .of(\"item\", Schema.FieldType.STRING)\n",
    "    .withDescription(\"Purchased Item\")\n",
    "    .withOptions(Schema.Options.builder().setOption(\"myFieldOption\", Schema.FieldType.STRING, \"Hello\")))\n",
    "  .addDoubleField(\"amount\")\n",
    "  .addDateTimeField(\"time\")\n",
    "  .setOptions(Schema.Options.builder().setOption(\"mySchemaOption\", Schema.FieldType.STRING, \"Hello\"))\n",
    "  .build();\n",
    "System.out.println(saleSchema);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9fbd59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.16+8-post-Debian-1deb10u1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
